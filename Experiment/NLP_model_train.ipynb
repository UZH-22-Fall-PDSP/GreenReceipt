{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Dependancies"
      ],
      "metadata": {
        "id": "W-FPUzzXVq4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mount google drive"
      ],
      "metadata": {
        "id": "iBKOuItAVwXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xit9YePaVRp_",
        "outputId": "cc0a600f-3499-4e9a-c815-d22cfab32407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/!_2022_fall/PDSP/kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc3uZakUVfwC",
        "outputId": "1a03465f-fadd-4c4f-b4ba-9d74ba31fdf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/!_2022_fall/PDSP/kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import packages"
      ],
      "metadata": {
        "id": "r_U2M7Y6Vy-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "import nltk\n",
        "from string import punctuation \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import WordPunctTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "from gensim.models import FastText\n"
      ],
      "metadata": {
        "id": "I26q0iexVlnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dataset preparation"
      ],
      "metadata": {
        "id": "E9koUiklV8N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load dataset"
      ],
      "metadata": {
        "id": "W8Xy3s-jWoYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read recipe csv file\n",
        "df_train = pd.read_csv(\"./data/RAW_recipes.csv\")\n",
        "\n",
        "# extract relevant col\n",
        "steps = df_train['steps']\n",
        "reviews = df_train['description']\n",
        "##ingredients = df_train['ingredients']"
      ],
      "metadata": {
        "id": "y7OMjdGyWDUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text normalization & tokenizing "
      ],
      "metadata": {
        "id": "xDSmIIJIWg9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(document):\n",
        "  # remove special characters\n",
        "  document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "  # remove numbers \n",
        "  document = re.sub('[0-9]+', '', document)\n",
        "\n",
        "  # remove single characters\n",
        "  document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "  document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "  # substituting multiple spaces with single space\n",
        "  document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "  # converting to lowercase\n",
        "  dodument = document.lower()\n",
        "\n",
        "  # tokenizing \n",
        "  document = tokenizer.tokenize(document) \n",
        "\n",
        "  # remove stop words \n",
        "  document = [w for w in document if len(w) > 2 if not w in en_stop]\n",
        "  \n",
        "  # lemmatization \n",
        "  stemmer = WordNetLemmatizer()\n",
        "  document = [stemmer.lemmatize(word) for word in document]\n",
        "  document = [word for word in document if word not in en_stop]\n",
        "  document = [word for word in document if len(word)>3]\n",
        "\n",
        "  return document"
      ],
      "metadata": {
        "id": "vk694nKAWZdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "tokenizer = nltk.WordPunctTokenizer()\n",
        "\n",
        "# preprocessing dataset \n",
        "preprocessed_steps = [preprocessing(step) for step in steps]\n",
        "preprocessed_reviews = [preprocessing(review) for review in reviews]\n"
      ],
      "metadata": {
        "id": "BZ7ihrfiWq4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge \n",
        "dataset = []\n",
        "dataset.extend(preprocessed_steps)\n",
        "dataset.extend(preprocessed_reviews)"
      ],
      "metadata": {
        "id": "aWpPt0Jmc999"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Train FastText model\n",
        "\n",
        "* quick tutorial : https://github.com/PacktPublishing/fastText-Quick-Start-Guide/blob/master/chapter5/fasttext%20with%20gensim.ipynb\n",
        "\n",
        "* learn more about the gensim fastText model parameter : https://radimrehurek.com/gensim/models/fasttext.html\n",
        "\n",
        "  * window (int, optional) â€“ The maximum distance between the current and predicted word within a sentence."
      ],
      "metadata": {
        "id": "RiohIKh5W9ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fooddotcom_v1`  \n",
        " * dataset : steps\n",
        " * vector size : 100\n",
        " * model : skip-gram \n",
        "   \n",
        "   \n",
        " `fooddotcom_v2`  \n",
        " * dataset : steps + review\n",
        " * vector size : 100\n",
        " * model : c-bow\n",
        " * n_gram min max"
      ],
      "metadata": {
        "id": "5yScCW-KXin2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model2 = FastText(dataset, size=100, window=5, min_count=5, min_n=2, max_n=5, workers=4, sg=1)"
      ],
      "metadata": {
        "id": "rvb8a_HSXEbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = FastText(dataset, size=200, window=5, min_count=5, min_n=2, max_n=5, workers=4, sg=0)"
      ],
      "metadata": {
        "id": "7kMB4qOrKUh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick test \n",
        "model3.wv.most_similar(\"piece scrod fillets fish choice\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewcwuZ_6YVqe",
        "outputId": "f4f17729-1f5c-4b3f-aa40-1fb2cab86c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fillet', 0.5782290697097778),\n",
              " ('pikelet', 0.5710306167602539),\n",
              " ('swordfish', 0.5619159936904907),\n",
              " ('catfish', 0.5538877248764038),\n",
              " ('piglet', 0.5432965159416199),\n",
              " ('filet', 0.5417371988296509),\n",
              " ('filleted', 0.5275126695632935),\n",
              " ('rockfish', 0.5269321203231812),\n",
              " ('piece', 0.525778591632843),\n",
              " ('monkfish', 0.5253928899765015)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model3.save('./model/fooddotcom_v3')"
      ],
      "metadata": {
        "id": "dsX-NoL6Y2B2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}